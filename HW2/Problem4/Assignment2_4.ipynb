{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each decimal output, the first number is MSE, the second one is Pearson correlation coefficient, and the thrid value is corresponding p-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import gensim\n",
    "import readability\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sl\n",
    "from nltk.parse import stanford\n",
    "from scipy import spatial\n",
    "from scipy.stats.stats import pearsonr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ruosen\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: The StanfordParser will be deprecated\n",
      "Please use \u001b[91mnltk.parse.corenlp.CoreNLPParser\u001b[0m instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "os.environ['STANFORD_PARSER'] = \"../Tools/stanford-parser-full-2018-10-17/stanford-parser.jar\"\n",
    "os.environ['STANFORD_MODELS'] = \"../Tools/stanford-parser-full-2018-10-17/stanford-parser-3.9.2-models.jar\"\n",
    "parser = stanford.StanfordParser( model_path = \"edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_excel( \"Data/Train_Data.xlsx\", sheet_name = \"Sheet1\" )\n",
    "data_test  = pd.read_excel( \"Data/Test_Data.xlsx\", sheet_name = \"Sheet1\" )\n",
    "stop_words = set( nltk.corpus.stopwords.words( \"english\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "                \"Data/GoogleNews-vectors-negative300.bin\", binary = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic data processing functions\n",
    "\"\"\"Clean data\n",
    "\n",
    "1. Replace tab or new line characters with space\n",
    "2. Lowercase words\n",
    "3. Remove extra spaces\n",
    "\n",
    "Args:\n",
    "    line: a string contains original sentence(s) or content(s).\n",
    "    \n",
    "Returns:\n",
    "    line: a string contains cleaned sentence(s) or content(s).\n",
    "\"\"\"\n",
    "def clean( line ):\n",
    "    line = re.sub( \"[\\sâ–ƒ]\", \" \", line )\n",
    "    line = re.sub( \"_\", \"\", line )\n",
    "    line = re.sub( \"[^\\w\\s]\", \"\", line )\n",
    "    line = re.sub( \"\\s+\", \" \", line ).strip()\n",
    "    line.lower()\n",
    "    return line\n",
    "\n",
    "\"\"\"Preprocess all data in summary level\n",
    "\n",
    "Args:\n",
    "    data: a dataframe contains Summary text, Non-Redundancy score,\n",
    "          and Fluency score.\n",
    "\n",
    "Returns:\n",
    "    new_summaries: a list of string which contains cleaned summary.\n",
    "\"\"\"\n",
    "def preprocess( data ):\n",
    "    print( \"Preprocessing...\" )\n",
    "    summaries = data[\"Summary\"]\n",
    "    new_summaries = []\n",
    "    for summary in summaries:\n",
    "        new_summaries.append( clean( summary ) )\n",
    "    return new_summaries\n",
    "\n",
    "\"\"\"Preprocess all data in sentence level\n",
    "\n",
    "Args:\n",
    "    data: a dataframe contains Summary text, Non-Redundancy score,\n",
    "          and Fluency score.\n",
    "\n",
    "Returns:\n",
    "    new_summaries: a list of cleaned summary. Its structure is:\n",
    "    \n",
    "                   [[sentence1], [sentence2], ...]\n",
    "\"\"\"\n",
    "def preprocess_sentence( data ):\n",
    "    # Split summaries into sentences and clean them\n",
    "    summaries = data[\"Summary\"]\n",
    "    new_summaries = []\n",
    "    for summary in summaries:\n",
    "        sentences = summary.split( '.' )\n",
    "        new_sentences = []\n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.strip()\n",
    "            new_sentence = clean( sentence )\n",
    "            if len( new_sentence ) == 0:\n",
    "                continue\n",
    "            new_sentences.append( new_sentence )\n",
    "        new_summaries.append( new_sentences )\n",
    "    return new_summaries\n",
    "\n",
    "\"\"\"Standardize data\n",
    "\n",
    "Args:\n",
    "    data: a numpy array.\n",
    "    cache: a dictionary contains cached mean and standard diviation.\n",
    "           Its structure is:\n",
    "           \n",
    "           {\"mean\": mean, \"std\": standard diviation}\n",
    "\n",
    "Returns:\n",
    "    std_data: a numpy data contains stardaized data.\n",
    "    cache: a dictionary contains cached mean and standard diviation.\n",
    "           Its structure is:\n",
    "           \n",
    "           {\"mean\": mean, \"std\": standard diviation}\n",
    "\"\"\"\n",
    "def standardize( data, cache = None ):\n",
    "    if isinstance( data, list ):\n",
    "        tdata = np.array( data )\n",
    "    if cache == None:\n",
    "        cache = {\"mean\": np.mean( tdata ),\n",
    "                 \"std\" : np.std ( tdata )}\n",
    "    std_data = ( tdata - cache[\"mean\"] ) / cache[\"std\"]\n",
    "    if isinstance( data, list ):\n",
    "        std_data = std_data.tolist()\n",
    "    return std_data, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils for problem 4.1\n",
    "\"\"\"Get max unigram of each sentence\n",
    "\n",
    "Maximum repetition of unigrams: calculate the frequencies of all unigrams\n",
    "(remove stop words), and use the maximum value as the result.\n",
    "\n",
    "Args:\n",
    "    data: a dataframe contains Summary text, Non-Redundancy score,\n",
    "          and Fluency score.\n",
    "    stop_words: a set of stop words.\n",
    "    cache: a dictionary contains cached mean and standard diviation.\n",
    "           Its structure is:\n",
    "           \n",
    "           {\"mean\": mean, \"std\": standard diviation}\n",
    "\n",
    "Returns:\n",
    "    unigrams: a list of max unigram of each sentence corresponding to\n",
    "              original sentences.\n",
    "    cache: a dictionary contains cached mean and standard diviation.\n",
    "           Its structure is:\n",
    "           \n",
    "           {\"mean\": mean, \"std\": standard diviation}\n",
    "\"\"\"\n",
    "def get_max_unigram( data, stop_words, cache = None ):\n",
    "    print( \"Geting maximum repetition of unigrams of each sentence...\" )\n",
    "    summaries = preprocess( data )\n",
    "    unigrams = []\n",
    "    for summary in summaries:\n",
    "        unigram = {}\n",
    "        words = summary.split()\n",
    "        max_number = 0\n",
    "        for word in words:\n",
    "            if word in stop_words:\n",
    "                continue\n",
    "            if word not in unigram:\n",
    "                unigram[word] = 0\n",
    "            unigram[word] += 1\n",
    "            max_number = max( max_number, unigram[word] )\n",
    "        unigrams.append( max_number )\n",
    "    unigrams, cache = standardize( unigrams, cache )\n",
    "    return unigrams, cache\n",
    "\n",
    "\"\"\"Get max bigram of each sentence\n",
    "\n",
    "Maximum repetition of bigrams: calculate the frequencies of all bigrams,\n",
    "and use the maximum value as the result.\n",
    "\n",
    "Args:\n",
    "    data: a dataframe contains Summary text, Non-Redundancy score,\n",
    "          and Fluency score.\n",
    "    cache: a dictionary contains cached mean and standard diviation.\n",
    "           Its structure is:\n",
    "           \n",
    "           {\"mean\": mean, \"std\": standard diviation}\n",
    "\n",
    "Returns:\n",
    "    bigrams: a list of max bigram of each sentence corresponding to\n",
    "            original sentences.\n",
    "    cache: a dictionary contains cached mean and standard diviation.\n",
    "           Its structure is:\n",
    "           \n",
    "           {\"mean\": mean, \"std\": standard diviation}\n",
    "\"\"\"\n",
    "def get_max_bigram( data, cache = None ):\n",
    "    print( \"Geting maximum repetition of bigrams of each sentence...\" )\n",
    "    summaries = preprocess( data )\n",
    "    bigrams = []\n",
    "    for summary in summaries:\n",
    "        bigram = {}\n",
    "        words = summary.split()\n",
    "        max_number = 0\n",
    "        prev_word = \"\"\n",
    "        for i in range( len( words ) - 1 ):\n",
    "            two_words = ' '.join( words[i: i + 2] )\n",
    "            if two_words not in bigram:\n",
    "                bigram[two_words] = 0\n",
    "            bigram[two_words] += 1\n",
    "            max_number = max( max_number, bigram[two_words] )\n",
    "        bigrams.append( max_number )\n",
    "    bigrams, cache = standardize( bigrams, cache )\n",
    "    return bigrams, cache\n",
    "\n",
    "\"\"\"Get max unigram of each sentence\n",
    "\n",
    "Maximum sentence similarity: each sentence is represented as average of\n",
    "word embeddings, then compute cosine similarity between pairwise sentences,\n",
    "use the maximum similarity as the result.\n",
    "\n",
    "Args:\n",
    "    data: a dataframe contains Summary text, Non-Redundancy score,\n",
    "          and Fluency score.\n",
    "    word2vec: a word2vec model obtained from nltk.\n",
    "    cache: a dictionary contains cached mean and standard diviation.\n",
    "           Its structure is:\n",
    "           \n",
    "           {\"mean\": mean, \"std\": standard diviation}\n",
    "\n",
    "Returns:\n",
    "    sen_sim: a list of max sentence similarity of each sentence.\n",
    "    cache: a dictionary contains cached mean and standard diviation.\n",
    "           Its structure is:\n",
    "           \n",
    "           {\"mean\": mean, \"std\": standard diviation}\n",
    "\"\"\"\n",
    "def get_max_sentence_similarity( data, word2vec, cache = None ):\n",
    "    print( \"Geting maximum sentence similarity of each sentence...\" )\n",
    "    \n",
    "    summaries = preprocess_sentence( data )\n",
    "    \n",
    "    # Calculate the average word embedding for sentences\n",
    "    word2vec_dict = word2vec.vocab.keys()\n",
    "    avgvecs_units = []\n",
    "    for summary in summaries:\n",
    "        avgvecs_sentences = []\n",
    "        for sentence in summary:\n",
    "            words = sentence.split()\n",
    "            # 300 dims in word2vec for each word-0vector\n",
    "            wordvecs = []\n",
    "            for word in words:\n",
    "                if word in word2vec_dict:\n",
    "                    wordvecs.append( word2vec[word] )\n",
    "            if len( wordvecs ) == 0:\n",
    "                continue\n",
    "            wordvecs = np.array( wordvecs )\n",
    "            avgvecs_sentences.append( np.mean( wordvecs, axis = 0 ) )\n",
    "        avgvecs_units.append( avgvecs_sentences )\n",
    "    \n",
    "    # Calculate max sentence similarity\n",
    "    print( \"Calculating cosine similarity...\" )\n",
    "    sen_sims = []\n",
    "    for avgvecs in avgvecs_units:\n",
    "        max_cos_sim = -100\n",
    "        for i in range( len( avgvecs ) ):\n",
    "            for j in range( len( avgvecs ) ):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                cos_sim = 1 - spatial.distance.cosine( avgvecs[i], avgvecs[j] )\n",
    "                max_cos_sim = max( max_cos_sim, cos_sim )\n",
    "        sen_sims.append( max_cos_sim )\n",
    "    sen_sims, cache = standardize( sen_sims, cache )\n",
    "    return sen_sims, cache\n",
    "\n",
    "\"\"\"Get length for each sentence\n",
    "\n",
    "Args:\n",
    "    data: a dataframe contains Summary text, Non-Redundancy score,\n",
    "          and Fluency score.\n",
    "    cache: a dictionary contains cached mean and standard diviation.\n",
    "           Its structure is:\n",
    "           \n",
    "           {\"mean\": mean, \"std\": standard diviation}\n",
    "\n",
    "Returns:\n",
    "    lengths: a list of length of each summary.\n",
    "    cache: a dictionary contains cached mean and standard diviation.\n",
    "           Its structure is:\n",
    "           \n",
    "           {\"mean\": mean, \"std\": standard diviation}\n",
    "\"\"\"\n",
    "def get_length( data, cache = None ):\n",
    "    print( \"Getting length of each sentence...\" )\n",
    "    summaries = preprocess( data )\n",
    "    lengths = []\n",
    "    for summary in summaries:\n",
    "        lengths.append( len( summary.split() ) )\n",
    "    lengths, cache = standardize( lengths, cache )\n",
    "    return lengths, cache\n",
    "\n",
    "\"\"\"Get ratio of stop words in each sentence\n",
    "\n",
    "Calculate ratio of stop words in each sentences.\n",
    "\n",
    "Args:\n",
    "    data: a dataframe contains Summary text, Non-Redundancy score,\n",
    "          and Fluency score.\n",
    "    stop_words: a set of stop words.\n",
    "    cache: a dictionary contains cached mean and standard diviation.\n",
    "           Its structure is:\n",
    "           \n",
    "           {\"mean\": mean, \"std\": standard diviation}\n",
    "\n",
    "Returns:\n",
    "    ratio_of_stop_words: a list of ratio of stop words in each\n",
    "                         sentence.\n",
    "    cache: a dictionary contains cached mean and standard diviation.\n",
    "           Its structure is:\n",
    "           \n",
    "           {\"mean\": mean, \"std\": standard diviation}\n",
    "\"\"\"\n",
    "def get_ratio_of_stop_words( data, stop_words, cache = None ):\n",
    "    print( \"Getting ratio of stop words of each sentence...\" )\n",
    "    summaries = preprocess( data )\n",
    "    ratio_of_stop_words = []\n",
    "    for summary in summaries:\n",
    "        words = summary.split()\n",
    "        cnt = 0\n",
    "        for word in words:\n",
    "            if word in stop_words:\n",
    "                cnt += 1\n",
    "        ratio_of_stop_words.append( cnt / len( word ) )\n",
    "    ratio_of_stop_words, cache = standardize( ratio_of_stop_words, cache )\n",
    "    return ratio_of_stop_words, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Train part of problem 4.1.1\n",
    "\n",
    "Args:\n",
    "    data: a dataframe contains Summary text, Non-Redundancy score,\n",
    "          and Fluency score.\n",
    "    stop_words: a set of stop words.\n",
    "    word2vec: a word2vec model obtained from nltk.\n",
    "\n",
    "Returns:\n",
    "    model: a well-trained mlp model.\n",
    "    caches: a dictionary contains cached mean and standard diviation\n",
    "            of all features. Its structure is:\n",
    "           \n",
    "            {\"unigram\": {\"mean\": mean, \"std\": standard diviation},\n",
    "             \"bigram\" : ...,\n",
    "             \"sen_sim\": ...}\n",
    "\"\"\"\n",
    "def problem4_1_1_train( data, stop_words, word2vec ):\n",
    "    print( \"Problem 4.1.1 Train\" )\n",
    "    print( \"Getting features...\" )\n",
    "    caches = {}\n",
    "    max_unigram, caches[\"unigram\"] = get_max_unigram( data, stop_words )\n",
    "    max_bigram,  caches[\"bigram\"]  = get_max_bigram( data )\n",
    "    max_sen_sim, caches[\"sen_sim\"] = get_max_sentence_similarity( data, word2vec )\n",
    "    features = list( zip( max_unigram, max_bigram, max_sen_sim ) )\n",
    "    labels = list( data[\"Non-Redundancy\"] )\n",
    "    print( \"Building and training models...\" )\n",
    "    model = MLPRegressor( ( 5, ), activation = \"tanh\",\n",
    "                          max_iter = 1000, learning_rate_init = 0.01 )\n",
    "    model.fit( features, labels )\n",
    "    return model, caches\n",
    "\n",
    "\"\"\"Test part of problem 4.1.1\n",
    "\n",
    "Args:\n",
    "    data: a dataframe contains Summary text, Non-Redundancy score,\n",
    "          and Fluency score.\n",
    "    model: a trained model.\n",
    "    stop_words: a set of stop words.\n",
    "    word2vec: a word2vec model obtained from nltk.\n",
    "    caches: a dictionary contains cached mean and standard diviation\n",
    "            of all features. Its structure is:\n",
    "           \n",
    "            {\"unigram\": {\"mean\": mean, \"std\": standard diviation},\n",
    "             \"bigram\" : ...,\n",
    "             \"sen_sim\": ...}\n",
    "\n",
    "Returns:\n",
    "    None.\n",
    "\"\"\"\n",
    "def problem4_1_1_test( data, model, stop_words, word2vec, caches ):\n",
    "    print( \"Problem 4.1.1 Test\" )\n",
    "    print( \"Getting features...\" )\n",
    "    max_unigram, _ = get_max_unigram( data, stop_words, caches[\"unigram\"] )\n",
    "    max_bigram,  _ = get_max_bigram( data, caches[\"bigram\"] )\n",
    "    max_sen_sim, _ = get_max_sentence_similarity( data, word2vec,\n",
    "                                                  caches[\"sen_sim\"] )\n",
    "    features = list( zip( max_unigram, max_bigram, max_sen_sim ) )\n",
    "    labels = list( data[\"Non-Redundancy\"] )\n",
    "    print( \"Predicting...\" )\n",
    "    preds = model.predict( features )\n",
    "    print( \"Measuring...\" )\n",
    "    mses = mean_squared_error( labels, preds )\n",
    "    pcor = pearsonr( labels, preds )\n",
    "    print( mses, pcor )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem 4.1.1 Train\n",
      "Getting features...\n",
      "Geting maximum repetition of unigrams of each sentence...\n",
      "Preprocessing...\n",
      "Geting maximum repetition of bigrams of each sentence...\n",
      "Preprocessing...\n",
      "Geting maximum sentence similarity of each sentence...\n",
      "Calculating cosine similarity...\n",
      "Building and training models...\n",
      "Problem 4.1.1 Test\n",
      "Getting features...\n",
      "Geting maximum repetition of unigrams of each sentence...\n",
      "Preprocessing...\n",
      "Geting maximum repetition of bigrams of each sentence...\n",
      "Preprocessing...\n",
      "Geting maximum sentence similarity of each sentence...\n",
      "Calculating cosine similarity...\n",
      "Predicting...\n",
      "Measuring...\n",
      "0.1779790558650628 (0.7447020756849336, 1.2806695318289553e-36)\n"
     ]
    }
   ],
   "source": [
    "model_4_1_1, caches = problem4_1_1_train( data_train, stop_words, word2vec )\n",
    "problem4_1_1_test( data_test, model_4_1_1, stop_words, word2vec, caches )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Train part of problem 4.1.2\n",
    "\n",
    "Args:\n",
    "    data: a dataframe contains Summary text, Non-Redundancy score,\n",
    "          and Fluency score.\n",
    "    stop_words: a set of stop words.\n",
    "    word2vec: a word2vec model obtained from nltk.\n",
    "    func: a string represents which function it would use.\n",
    "\n",
    "Returns:\n",
    "    model: a well-trained MLP model.\n",
    "    caches: a dictionary contains cached mean and standard diviation\n",
    "            of all features. Its structure is:\n",
    "           \n",
    "            {\"unigram\": {\"mean\": mean, \"std\": standard diviation},\n",
    "             \"bigram\" : ...,\n",
    "             \"sen_sim\": ...,\n",
    "             \"length\" : ...,\n",
    "             \"ratio\"  : ...}\n",
    "\"\"\"\n",
    "def problem4_1_2_train( data, stop_words, word2vec, func ):\n",
    "    print( \"Problem 4.1.2 Train\" )\n",
    "    print( \"Getting features...\" )\n",
    "    caches = {}\n",
    "    max_unigram, caches[\"unigram\"] = get_max_unigram( data, stop_words )\n",
    "    max_bigram,  caches[\"bigram\"]  = get_max_bigram( data )\n",
    "    max_sen_sim, caches[\"sen_sim\"] = get_max_sentence_similarity( data, word2vec )\n",
    "    if func == \"length\":\n",
    "        length, caches[\"length\"] = get_length( data )\n",
    "        features = list( zip( max_unigram, max_bigram, max_sen_sim, length ) )\n",
    "    else:\n",
    "        ratio,  caches[\"ratio\"]  = get_ratio_of_stop_words( data, stop_words )\n",
    "        features = list( zip( max_unigram, max_bigram, max_sen_sim, ratio ) )\n",
    "    labels = list( data[\"Non-Redundancy\"] )\n",
    "    print( \"Building and training models...\" )\n",
    "    model = MLPRegressor( ( 5, ), activation = \"tanh\",\n",
    "                           max_iter = 1000, learning_rate_init = 0.01 )\n",
    "    model.fit( features, labels )\n",
    "    return model, caches\n",
    "\n",
    "\"\"\"Test part of problem 4.1.2\n",
    "\n",
    "Args:\n",
    "    data: a dataframe contains Summary text, Non-Redundancy score,\n",
    "          and Fluency score.\n",
    "    model: a trained model.\n",
    "    stop_words: a set of stop words.\n",
    "    word2vec: a word2vec model obtained from nltk.\n",
    "    func: a string represents which function it would use.\n",
    "    caches: a dictionary contains cached mean and standard diviation\n",
    "            of all features. Its structure is:\n",
    "           \n",
    "            {\"unigram\": {\"mean\": mean, \"std\": standard diviation},\n",
    "             \"bigram\" : ...,\n",
    "             \"sen_sim\": ...,\n",
    "             \"length\" : ...,\n",
    "             \"ratio\"  : ...}\n",
    "\n",
    "Returns:\n",
    "    None.\n",
    "\"\"\"\n",
    "def problem4_1_2_test( data, model, stop_words, word2vec, func, caches ):\n",
    "    print( \"Problem 4.1.2 Test\" )\n",
    "    print( \"Getting features...\" )\n",
    "    max_unigram, _ = get_max_unigram( data, stop_words, caches[\"unigram\"] )\n",
    "    max_bigram,  _ = get_max_bigram( data, caches[\"bigram\"] )\n",
    "    max_sen_sim, _ = get_max_sentence_similarity( data, word2vec,\n",
    "                                                  caches[\"sen_sim\"] )\n",
    "    if func == \"length\":\n",
    "        length, _ = get_length( data, caches[\"length\"] )\n",
    "        features  = list( zip( max_unigram, max_bigram, max_sen_sim, length ) )\n",
    "    else:\n",
    "        ratio, _ = get_ratio_of_stop_words( data, stop_words, caches[\"ratio\"] )\n",
    "        features = list( zip( max_unigram, max_bigram, max_sen_sim, ratio ) )\n",
    "    labels = list( data[\"Non-Redundancy\"] )\n",
    "    print( \"Predicting by model...\" )\n",
    "    preds = model.predict( features )\n",
    "    print( \"Measuring...\" )\n",
    "    mses = mean_squared_error( labels, preds )\n",
    "    pcor = pearsonr( labels, preds )\n",
    "    print( mses, pcor )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Try to add feature length\n",
      "Problem 4.1.2 Train\n",
      "Getting features...\n",
      "Geting maximum repetition of unigrams of each sentence...\n",
      "Preprocessing...\n",
      "Geting maximum repetition of bigrams of each sentence...\n",
      "Preprocessing...\n",
      "Geting maximum sentence similarity of each sentence...\n",
      "Calculating cosine similarity...\n",
      "Getting length of each sentence...\n",
      "Preprocessing...\n",
      "Building and training models...\n",
      "Problem 4.1.2 Test\n",
      "Getting features...\n",
      "Geting maximum repetition of unigrams of each sentence...\n",
      "Preprocessing...\n",
      "Geting maximum repetition of bigrams of each sentence...\n",
      "Preprocessing...\n",
      "Geting maximum sentence similarity of each sentence...\n",
      "Calculating cosine similarity...\n",
      "Getting length of each sentence...\n",
      "Preprocessing...\n",
      "Predicting by model...\n",
      "Measuring...\n",
      "0.17563927750208272 (0.7513204661151712, 1.371308915137041e-37)\n",
      "Try to add feature stop_words\n",
      "Problem 4.1.2 Train\n",
      "Getting features...\n",
      "Geting maximum repetition of unigrams of each sentence...\n",
      "Preprocessing...\n",
      "Geting maximum repetition of bigrams of each sentence...\n",
      "Preprocessing...\n",
      "Geting maximum sentence similarity of each sentence...\n",
      "Calculating cosine similarity...\n",
      "Getting ratio of stop words of each sentence...\n",
      "Preprocessing...\n",
      "Building and training models...\n",
      "Problem 4.1.2 Test\n",
      "Getting features...\n",
      "Geting maximum repetition of unigrams of each sentence...\n",
      "Preprocessing...\n",
      "Geting maximum repetition of bigrams of each sentence...\n",
      "Preprocessing...\n",
      "Geting maximum sentence similarity of each sentence...\n",
      "Calculating cosine similarity...\n",
      "Getting ratio of stop words of each sentence...\n",
      "Preprocessing...\n",
      "Predicting by model...\n",
      "Measuring...\n",
      "0.17622671038797363 (0.7464431823882691, 7.163063138022386e-37)\n"
     ]
    }
   ],
   "source": [
    "func = \"length\"\n",
    "print( \"Try to add feature \" + func )\n",
    "model_4_1_2_1, caches = problem4_1_2_train( data_train, stop_words, word2vec, func )\n",
    "problem4_1_2_test( data_test, model_4_1_2_1, stop_words, word2vec, func, caches )\n",
    "func = \"stop_words\"\n",
    "print( \"Try to add feature \" + func )\n",
    "model_4_1_2_2, caches = problem4_1_2_train( data_train, stop_words, word2vec, func )\n",
    "problem4_1_2_test( data_test, model_4_1_2_2, stop_words, word2vec, func, caches )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils for problem 4.2\n",
    "\"\"\"Get number of repetitive unigram\n",
    "\n",
    "Args:\n",
    "    data: a dataframe contains Summary text, Non-Redundancy score,\n",
    "          and Fluency score.\n",
    "    cache: a dictionary contains cached mean and standard diviation.\n",
    "           Its structure is:\n",
    "           \n",
    "           {\"mean\": mean, \"std\": standard diviation}\n",
    "\n",
    "Returns:\n",
    "    cnts: a list contains stardardized count value of all sentences.\n",
    "    cache: a dictionary contains cached mean and standard diviation.\n",
    "           Its structure is:\n",
    "           \n",
    "           {\"mean\": mean, \"std\": standard diviation}\n",
    "\"\"\"\n",
    "def get_no_of_repetitive_unigram( data, cache = None ):\n",
    "    cnts = []\n",
    "    summaries = preprocess( data )\n",
    "    for summary in summaries:\n",
    "        words = summary.split()\n",
    "        unigram = {}\n",
    "        cnt = 0\n",
    "        for word in words:\n",
    "            if word not in unigram:\n",
    "                unigram[word] = 0\n",
    "            if unigram[word]:\n",
    "                cnt += 1\n",
    "            unigram[word] += 1\n",
    "        cnts.append( cnt )\n",
    "    cnts, cache = standardize( cnts, cache )\n",
    "    return cnts, cache\n",
    "\n",
    "\"\"\"Get number of repetitive bigram\n",
    "\n",
    "Args:\n",
    "    data: a dataframe contains Summary text, Non-Redundancy score,\n",
    "          and Fluency score.\n",
    "    cache: a dictionary contains cached mean and standard diviation.\n",
    "           Its structure is:\n",
    "           \n",
    "           {\"mean\": mean, \"std\": standard diviation}\n",
    "\n",
    "Returns:\n",
    "    cnts: a list contains stardardized count value of all sentences.\n",
    "    cache: a dictionary contains cached mean and standard diviation.\n",
    "           Its structure is:\n",
    "           \n",
    "           {\"mean\": mean, \"std\": standard diviation}\n",
    "\"\"\"\n",
    "def get_no_of_repetitive_bigram( data, cache = None ):\n",
    "    cnts = []\n",
    "    summaries = preprocess( data )\n",
    "    for summary in summaries:\n",
    "        words = summary.split()\n",
    "        bigram = {}\n",
    "        cnt = 0\n",
    "        for i in range( len( words ) ):\n",
    "            two_words = ' '.join( words[i:i + 2] )\n",
    "            if two_words not in bigram:\n",
    "                bigram[two_words] = 0\n",
    "            if bigram[two_words]:\n",
    "                cnt += 1\n",
    "            bigram[two_words] += 1\n",
    "        cnts.append( cnt )\n",
    "    cnts, cache = standardize( cnts, cache )\n",
    "    return cnts, cache\n",
    "\n",
    "\"\"\"Get easy read score\n",
    "\n",
    "Args:\n",
    "    data: a dataframe contains Summary text, Non-Redundancy score,\n",
    "          and Fluency score.\n",
    "    cache: a dictionary contains cached mean and standard diviation.\n",
    "           Its structure is:\n",
    "           \n",
    "           {\"mean\": mean, \"std\": standard diviation}\n",
    "\n",
    "Returns:\n",
    "    scores: a list contains stardardized scores of all sentences.\n",
    "    cache: a dictionary contains cached mean and standard diviation.\n",
    "           Its structure is:\n",
    "           \n",
    "           {\"mean\": mean, \"std\": standard diviation}\n",
    "\"\"\"\n",
    "def get_easy_read_score( data, cache = None ):\n",
    "    scores = []\n",
    "    summaries = preprocess_sentence( data )\n",
    "    for sentences in summaries:\n",
    "        min_score = 10000\n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.strip()\n",
    "            if len( sentence ) == 0:\n",
    "                continue\n",
    "            words = sentence.split()\n",
    "            score = readability.getmeasures( words, lang = 'en' )\n",
    "            score = score['readability grades']['FleschReadingEase']\n",
    "            min_score = min( min_score, score )\n",
    "        scores.append( min_score )\n",
    "    scores, cache = standardize( scores, cache )\n",
    "    return scores, cache\n",
    "\n",
    "\"\"\"Get parser height of all sentences\n",
    "\n",
    "Args:\n",
    "    data: a dataframe contains Summary text, Non-Redundancy score,\n",
    "          and Fluency score.\n",
    "    parser: a stanford parser for parsing english PENN tree.\n",
    "    cache: a dictionary contains cached mean and standard diviation.\n",
    "           Its structure is:\n",
    "           \n",
    "           {\"mean\": mean, \"std\": standard diviation}\n",
    "\n",
    "Returns:\n",
    "    heights: a list contains stardardized heights of all sentences.\n",
    "    cache: a dictionary contains cached mean and standard diviation.\n",
    "           Its structure is:\n",
    "           \n",
    "           {\"mean\": mean, \"std\": standard diviation}\n",
    "\"\"\"\n",
    "def get_parser_height( data, parser, cache = None ):\n",
    "    summaries = preprocess_sentence( data )\n",
    "    heights = []\n",
    "    cnt = 0\n",
    "    for sentences in summaries:\n",
    "        height = []\n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.strip()\n",
    "            if len( sentence ) == 0:\n",
    "                continue\n",
    "            parsed_sentence = parser.parse_sents( sentence )\n",
    "            height.append( next( next( parsed_sentence ) ).height () )\n",
    "            cnt += 1\n",
    "            print( \"Parsed sentences: \" + str( cnt ), end = \"\\r\" )\n",
    "        heights.append( np.mean( height ) )\n",
    "    print( \"\" )\n",
    "    heights, cache = standardize( heights, cache )\n",
    "    return heights, cache\n",
    "\n",
    "\"\"\"Get phrase propotion of all sentences\n",
    "\n",
    "For each summary, calculate the ratio of all pos-tags starts with \"IN\",\n",
    "\"NN\", or \"VB\".\n",
    "\n",
    "Args:\n",
    "    data: a dataframe contains Summary text, Non-Redundancy score,\n",
    "          and Fluency score.\n",
    "    cache: a dictionary contains cached mean and standard diviation.\n",
    "           Its structure is:\n",
    "           \n",
    "           {\"mean\": mean, \"std\": standard diviation}\n",
    "\n",
    "Returns:\n",
    "    phrase_ratio: a list contains stardardized phrase ratio of all\n",
    "                  sentences.\n",
    "    cache: a dictionary contains cached mean and standard diviation.\n",
    "           Its structure is:\n",
    "           \n",
    "           {\"mean\": mean, \"std\": standard diviation}\n",
    "\"\"\"\n",
    "def get_phrase_propotion( data, cache = None ):\n",
    "    summaries = preprocess_sentence( data )\n",
    "    phrase_ratio = []\n",
    "    total_cnt = 0\n",
    "    for sentences in summaries:\n",
    "        ratio = []\n",
    "        cnt = 0\n",
    "        cnt_tag = 0\n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.strip()\n",
    "            if len( sentence ) == 0:\n",
    "                continue\n",
    "            pos_tags = nltk.pos_tag( sentence )\n",
    "            for pos_tag in pos_tags:\n",
    "                cnt += 1\n",
    "                if pos_tag[1].startswith( \"IN\" ) or \\\n",
    "                        pos_tag[1].startswith( \"NN\" ) or \\\n",
    "                        pos_tag[1].startswith( \"VB\" ):\n",
    "                    cnt_tag += 1\n",
    "            total_cnt += 1\n",
    "            print( \"Parsed sentence: \" + str( total_cnt ), end = \"\\r\" )\n",
    "        phrase_ratio.append( cnt_tag / cnt )\n",
    "    print( \"\" )\n",
    "    phrase_ratio, cache = standardize( phrase_ratio, cache )\n",
    "    return phrase_ratio, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Train part of problem 4.2.1\n",
    "\n",
    "Args:\n",
    "    data: a dataframe contains Summary text, Non-Redundancy score,\n",
    "          and Fluency score.\n",
    "    stop_words: a set of stop words.\n",
    "\n",
    "Returns:\n",
    "    model: a well-trained MLP model.\n",
    "    caches: a dictionary contains cached mean and standard diviation\n",
    "            of all features. Its structure is:\n",
    "           \n",
    "            {\"rep_unigram\": {\"mean\": mean, \"std\": standard diviation},\n",
    "             \"rep_bigram\" : ...,\n",
    "             \"rd_score\"   : ...}\n",
    "\"\"\"\n",
    "def problem4_2_1_train( data, stop_words ):\n",
    "    print( \"Problem 4.2.1 Train\" )\n",
    "    print( \"Getting features...\" )\n",
    "    caches = {}\n",
    "    rep_uni,  caches[\"rep_unigram\"] = get_no_of_repetitive_unigram( data )\n",
    "    rep_bi,   caches[\"rep_bigram\"]  = get_no_of_repetitive_bigram( data )\n",
    "    rd_score, caches[\"rd_score\"]    = get_easy_read_score( data )\n",
    "    features = list( zip( rep_uni, rep_bi, rd_score ) )\n",
    "    labels = list( data[\"Fluency\"] )\n",
    "    print( \"Building and training models...\" )\n",
    "    model = MLPRegressor( ( 5, ), activation = \"tanh\",\n",
    "                          max_iter = 1000, learning_rate_init = 0.01 )\n",
    "    model.fit( features, labels )\n",
    "    return model, caches\n",
    "\n",
    "\"\"\"Test part of problem 4.2.1\n",
    "\n",
    "Args:\n",
    "    data: a dataframe contains Summary text, Non-Redundancy score,\n",
    "          and Fluency score.\n",
    "    model: a trained model.\n",
    "    stop_words: a set of stop words.\n",
    "    caches: a dictionary contains cached mean and standard diviation\n",
    "            of all features. Its structure is:\n",
    "           \n",
    "            {\"rep_unigram\": {\"mean\": mean, \"std\": standard diviation},\n",
    "             \"rep_bigram\" : ...,\n",
    "             \"rd_score\"   : ...}\n",
    "\n",
    "Returns:\n",
    "    None.\n",
    "\"\"\"\n",
    "def problem4_2_1_test( data, model, stop_words, caches ):\n",
    "    print( \"Problem 4.2.1 Test\" )\n",
    "    print( \"Getting features...\" )\n",
    "    rep_uni,  _ = get_no_of_repetitive_unigram( data, caches[\"rep_unigram\"] )\n",
    "    rep_bi,   _ = get_no_of_repetitive_bigram ( data, caches[\"rep_bigram\"] )\n",
    "    rd_score, _ = get_easy_read_score( data, caches[\"rd_score\"] )\n",
    "    features = list( zip( rep_uni, rep_bi, rd_score ) )\n",
    "    labels = list( data[\"Fluency\"] )\n",
    "    print( \"Predicting...\" )\n",
    "    preds = model.predict( features )\n",
    "    print( \"Measuring...\" )\n",
    "    mses = mean_squared_error( labels, preds )\n",
    "    pcor = pearsonr( labels, preds )\n",
    "    print( mses, pcor )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem 4.2.1 Train\n",
      "Getting features...\n",
      "Preprocessing...\n",
      "Preprocessing...\n",
      "Building and training models...\n",
      "Problem 4.2.1 Test\n",
      "Getting features...\n",
      "Preprocessing...\n",
      "Preprocessing...\n",
      "Predicting...\n",
      "Measuring...\n",
      "0.23833074210188449 (0.29134845986416164, 2.8437665383259195e-05)\n"
     ]
    }
   ],
   "source": [
    "model_4_2_1, caches = problem4_2_1_train( data_train, stop_words )\n",
    "problem4_2_1_test( data_test, model_4_2_1, stop_words, caches )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Train part of problem 4.2.2\n",
    "\n",
    "Args:\n",
    "    data: a dataframe contains Summary text, Non-Redundancy score,\n",
    "          and Fluency score.\n",
    "    stop_words: a set of stop words.\n",
    "    parser: a stanford parser for parsing english PENN tree.\n",
    "    func: a string represents which function it would use.\n",
    "    caches: a dictionary contains cached mean and standard diviation\n",
    "            of all features. Its structure is:\n",
    "           \n",
    "            {\"rep_unigram\": {\"mean\": mean, \"std\": standard diviation},\n",
    "             \"rep_bigram\" : ...,\n",
    "             \"rd_score\"   : ...,\n",
    "             \"parser_height\": ...,\n",
    "             \"phrase_prop\"  : ...}\n",
    "\n",
    "Returns:\n",
    "    model: a well-trained MLP model.\n",
    "\"\"\"\n",
    "def problem4_2_2_train( data, stop_words, parser, func ):\n",
    "    print( \"Problem 4.2.2 Train\" )\n",
    "    print( \"Getting features...\" )\n",
    "    caches = {}\n",
    "    rep_unigram, caches[\"rep_unigram\"] = get_no_of_repetitive_unigram( data )\n",
    "    rep_bigram,  caches[\"rep_bigram\"]  = get_no_of_repetitive_bigram ( data )\n",
    "    rd_score,    caches[\"rd_score\"]    = get_easy_read_score( data )\n",
    "    if func == \"height\":\n",
    "        parser_height, caches[\"parser_height\"] = get_parser_height( data, parser )\n",
    "        features = list( zip( rep_unigram, rep_bigram, rd_score, parser_height ) )\n",
    "    else:\n",
    "        phrase_prop, caches[\"phrase_prop\"] = get_phrase_propotion( data )\n",
    "        features = list( zip( rep_unigram, rep_bigram, rd_score, phrase_prop ) )\n",
    "    labels = list( data[\"Non-Redundancy\"] )\n",
    "    print( \"Building and training models...\" )\n",
    "    model = MLPRegressor( ( 5, ), activation = \"tanh\",\n",
    "                           max_iter = 1000, learning_rate_init = 0.01 )\n",
    "    model.fit( features, labels )\n",
    "    return model, caches\n",
    "\n",
    "\"\"\"Test part of problem 4.2.2\n",
    "\n",
    "Args:\n",
    "    data: a dataframe contains Summary text, Non-Redundancy score,\n",
    "          and Fluency score.\n",
    "    model: a trained model.\n",
    "    stop_words: a set of stop words.\n",
    "    parser: a stanford parser for parsing english PENN tree.\n",
    "    func: a string represents which function it would use.\n",
    "    caches: a dictionary contains cached mean and standard diviation\n",
    "            of all features. Its structure is:\n",
    "           \n",
    "            {\"rep_unigram\": {\"mean\": mean, \"std\": standard diviation},\n",
    "             \"rep_bigram\" : ...,\n",
    "             \"rd_score\"   : ...,\n",
    "             \"parser_height\": ...,\n",
    "             \"phrase_prop\"  : ...}\n",
    "\n",
    "Returns:\n",
    "    None.\n",
    "\"\"\"\n",
    "def problem4_2_2_test( data, model, stop_words, parser, func, caches ):\n",
    "    print( \"Problem 4.2.2 Test\" )\n",
    "    print( \"Getting features...\" )\n",
    "    rep_unigram, _ = get_no_of_repetitive_unigram( data, caches[\"rep_unigram\"] )\n",
    "    rep_bigram,  _ = get_no_of_repetitive_bigram ( data, caches[\"rep_bigram\"] )\n",
    "    rd_score,    _ = get_easy_read_score( data, caches[\"rd_score\"] )\n",
    "    if func == \"height\":\n",
    "        parser_height, _ = get_parser_height( data, parser, caches[\"parser_height\"] )\n",
    "        features = list( zip( rep_unigram, rep_bigram, rd_score, parser_height ) )\n",
    "    else:\n",
    "        phrase_prop, _ = get_phrase_propotion( data, caches[\"phrase_prop\"] )\n",
    "        features = list( zip( rep_unigram, rep_bigram, rd_score, phrase_prop ) )\n",
    "    labels = list( data[\"Non-Redundancy\"] )\n",
    "    print( \"Predicting by model...\" )\n",
    "    preds = model.predict( features )\n",
    "    print( \"Measuring...\" )\n",
    "    mses = mean_squared_error( labels, preds )\n",
    "    pcor = pearsonr( labels, preds )\n",
    "    print( mses, pcor )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Try to add feature height\n",
      "Problem 4.2.2 Train\n",
      "Getting features...\n",
      "Preprocessing...\n",
      "Preprocessing...\n",
      "Parsed sentences: 2969\n",
      "Building and training models...\n",
      "Problem 4.2.2 Test\n",
      "Getting features...\n",
      "Preprocessing...\n",
      "Preprocessing...\n",
      "Parsed sentences: 753\n",
      "Predicting by model...\n",
      "Measuring...\n",
      "0.20144800022429155 (0.7026233600234025, 4.387885956980907e-31)\n",
      "Try to add feature phrase ratio\n",
      "Problem 4.2.2 Train\n",
      "Getting features...\n",
      "Preprocessing...\n",
      "Preprocessing...\n",
      "Parsed sentence: 2969\n",
      "Building and training models...\n",
      "Problem 4.2.2 Test\n",
      "Getting features...\n",
      "Preprocessing...\n",
      "Preprocessing...\n",
      "Parsed sentence: 753\n",
      "Predicting by model...\n",
      "Measuring...\n",
      "0.2007871260613363 (0.7042513146699327, 2.7947393979652166e-31)\n"
     ]
    }
   ],
   "source": [
    "func = \"height\"\n",
    "print( \"Try to add feature \" + func )\n",
    "model_4_2_2_1, caches = problem4_2_2_train( data_train, stop_words, parser, func )\n",
    "problem4_2_2_test( data_test, model_4_2_2_1, stop_words, parser, func, caches )\n",
    "func = \"phrase ratio\"\n",
    "print( \"Try to add feature \" + func )\n",
    "model_4_2_2_2, caches = problem4_2_2_train( data_train, stop_words, parser, func )\n",
    "problem4_2_2_test( data_test, model_4_2_2_2, stop_words, parser, func, caches )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
