{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sl\n",
    "from scipy import spatial\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_excel( \"Data/Train_Data.xlsx\", sheet_name = \"Sheet1\" )\n",
    "data_test  = pd.read_excel( \"Data/Test_Data.xlsx\", sheet_name = \"Sheet1\" )\n",
    "stop_words = set( nltk.corpus.stopwords.words( \"english\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "                \"Data/GoogleNews-vectors-negative300.bin\", binary = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Preprocess all data\n",
    "\n",
    "1. Replace tab or new line characters with space\n",
    "2. Lowercase words\n",
    "3. Remove extra spaces\n",
    "\n",
    "Args:\n",
    "    data: a dataframe contains Summary text, Non-Redundancy score,\n",
    "          and Fluency score.\n",
    "\n",
    "Returns:\n",
    "    None.\n",
    "\"\"\"\n",
    "def preprocessing( data ):\n",
    "    print( \"Preprocessing...\" )\n",
    "    summaries = data[\"Summary\"]\n",
    "    new_summaries = []\n",
    "    for summary in summaries:\n",
    "        summary = re.sub( \"[\\sâ–ƒ]\", \" \", summary )\n",
    "        summary = re.sub( \"_\", \"\", summary )\n",
    "        summary = re.sub( \"[^\\w\\s]\", \"\", summary )\n",
    "        summary = re.sub( \"\\s+\", \" \", summary ).strip()\n",
    "        summary.lower()\n",
    "        new_summaries.append( summary )\n",
    "    data[\"Summary\"] = new_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Get max unigram of each sentence\n",
    "\n",
    "Maximum repetition of unigrams: calculate the frequencies of all unigrams\n",
    "(remove stop words), and use the maximum value as the result.\n",
    "\n",
    "Args:\n",
    "    data: a dataframe contains Summary text, Non-Redundancy score,\n",
    "          and Fluency score.\n",
    "    stop_words: a set of stop words.\n",
    "\n",
    "Returns:\n",
    "    unigrams: a list of max unigram of each sentence corresponding to\n",
    "              original sentences.\n",
    "\"\"\"\n",
    "def get_max_unigram( data, stop_words ):\n",
    "    print( \"Geting maximum repetition of unigrams of each sentence...\" )\n",
    "    summaries = data[\"Summary\"]\n",
    "    unigrams = []\n",
    "    for summary in summaries:\n",
    "        unigram = {}\n",
    "        words = summary.split()\n",
    "        max_number = 0\n",
    "        for word in words:\n",
    "            if word in stop_words:\n",
    "                continue\n",
    "            if word not in unigram:\n",
    "                unigram[word] = 0\n",
    "            unigram[word] += 1\n",
    "            max_number = max( max_number, unigram[word] )\n",
    "        unigrams.append( max_number )\n",
    "    unigrams = np.array( unigrams )\n",
    "    unigrams = ( unigrams - np.mean( unigrams ) ) / np.std( unigrams )\n",
    "    unigrams = unigrams.tolist()\n",
    "    return unigrams\n",
    "\n",
    "\"\"\"Get max bigram of each sentence\n",
    "\n",
    "Maximum repetition of bigrams: calculate the frequencies of all bigrams,\n",
    "and use the maximum value as the result.\n",
    "\n",
    "Args:\n",
    "    data: a dataframe contains Summary text, Non-Redundancy score,\n",
    "          and Fluency score.\n",
    "    stop_words: a set of stop words.\n",
    "\n",
    "Returns:\n",
    "    bigrams: a list of max bigram of each sentence corresponding to\n",
    "            original sentences.\n",
    "\"\"\"\n",
    "def get_max_bigram( data, stop_words ):\n",
    "    print( \"Geting maximum repetition of bigrams of each sentence...\" )\n",
    "    summaries = data[\"Summary\"]\n",
    "    bigrams = []\n",
    "    for summary in summaries:\n",
    "        bigram = {}\n",
    "        words = summary.split()\n",
    "        max_number = 0\n",
    "        prev_word = \"\"\n",
    "        for word in words:\n",
    "            if word in stop_words:\n",
    "                continue\n",
    "            if prev_word == \"\":\n",
    "                prev_word = word\n",
    "                continue\n",
    "            two_words = ' '.join( [prev_word, word] )\n",
    "            if two_words not in bigram:\n",
    "                bigram[two_words] = 0\n",
    "            bigram[two_words] += 1\n",
    "            max_number = max( max_number, bigram[two_words] )\n",
    "            prev_word = word\n",
    "        bigrams.append( max_number )\n",
    "    bigrams = np.array( bigrams )\n",
    "    bigrams = ( bigrams - np.mean( bigrams ) ) / np.std( bigrams )\n",
    "    bigrams = bigrams.tolist()\n",
    "    return bigrams\n",
    "\n",
    "\"\"\"Get max unigram of each sentence\n",
    "\n",
    "Maximum sentence similarity: each sentence is represented as average of\n",
    "word embeddings, then compute cosine similarity between pairwise sentences,\n",
    "use the maximum similarity as the result.\n",
    "\n",
    "Args:\n",
    "    data: a dataframe contains Summary text, Non-Redundancy score,\n",
    "          and Fluency score.\n",
    "    stop_words: a set of stop words.\n",
    "    word2vec: a word2vec model obtained from nltk.\n",
    "\n",
    "Returns:\n",
    "    sen_sim: a list of max sentence similarity of each sentence.\n",
    "\"\"\"\n",
    "def get_max_sentence_similarity( data, stop_words, word2vec ):\n",
    "    print( \"Geting maximum sentence similarity of each sentence...\" )\n",
    "    summaries = data[\"Summary\"]\n",
    "    word2vec_dict = word2vec.vocab.keys()\n",
    "    sen_sim = []\n",
    "    avgvecs = []\n",
    "    for summary in summaries:\n",
    "        words = summary.split()\n",
    "        # 300 dims in word2vec for each word-0vector\n",
    "        wordvecs = []\n",
    "        for word in words:\n",
    "            if word not in word2vec_dict:\n",
    "                continue\n",
    "            wordvecs.append( word2vec[word] )\n",
    "        wordvecs = np.array( wordvecs )\n",
    "        avgvecs.append( np.mean( wordvecs, axis = 0 ) )\n",
    "    print( \"Calculating cosine similarity...\" )\n",
    "    for i in range( len( avgvecs ) ):\n",
    "        max_cos_sim = -100\n",
    "        for j in range( len( avgvecs ) ):\n",
    "            if i == j:\n",
    "                continue\n",
    "            cos_sim = 1 - spatial.distance.cosine( avgvecs[i], avgvecs[j] )\n",
    "            max_cos_sim = max( max_cos_sim, cos_sim )\n",
    "        sen_sim.append( max_cos_sim )\n",
    "    return sen_sim\n",
    "\n",
    "\"\"\"Get length for each sentence\n",
    "\n",
    "Args:\n",
    "    data: a dataframe contains Summary text, Non-Redundancy score,\n",
    "          and Fluency score.\n",
    "\n",
    "Returns:\n",
    "    lengths: a list of length of each summary.\n",
    "\"\"\"\n",
    "def get_length( data ):\n",
    "    print( \"Getting length of each sentence...\" )\n",
    "    summaries = data[\"Summary\"]\n",
    "    lengths = []\n",
    "    for summary in summaries:\n",
    "        lengths.append( len( summary ) )\n",
    "    return lengths\n",
    "\n",
    "\"\"\"Get ratio of stop words in each sentence\n",
    "\n",
    "Calculate ratio of stop words in each sentences.\n",
    "\n",
    "Args:\n",
    "    data: a dataframe contains Summary text, Non-Redundancy score,\n",
    "          and Fluency score.\n",
    "    stop_words: a set of stop words.\n",
    "\n",
    "Returns:\n",
    "    ratio_of_stop_words: a list of ratio of stop words in each\n",
    "                         sentence.\n",
    "\"\"\"\n",
    "def get_ratio_of_stop_words( data, stop_words ):\n",
    "    print( \"Getting ratio of stop words of each sentence...\" )\n",
    "    summaries = data[\"Summary\"]\n",
    "    ratio_of_stop_words = []\n",
    "    for summary in summaries:\n",
    "        words = summary.split()\n",
    "        cnt = 0\n",
    "        for word in words:\n",
    "            if word in stop_words:\n",
    "                cnt += 1\n",
    "        ratio_of_stop_words.append( cnt / len( word ) )\n",
    "    return ratio_of_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Train part of problem 4.1.1\n",
    "\n",
    "Args:\n",
    "    data: a dataframe contains Summary text, Non-Redundancy score,\n",
    "          and Fluency score.\n",
    "    stop_words: a set of stop words.\n",
    "    word2vec: a word2vec model obtained from nltk.\n",
    "\n",
    "Returns:\n",
    "    None.\n",
    "\"\"\"\n",
    "def problem4_1_1_train( data, stop_words, word2vec ):\n",
    "    print( \"Problem 4.1.1 Train\" )\n",
    "    print( \"Getting features...\" )\n",
    "    features = list( zip( get_max_unigram( data, stop_words ),\n",
    "                          get_max_bigram( data, stop_words ),\n",
    "                          get_max_sentence_similarity( data, stop_words,\n",
    "                                                       word2vec ) ) )\n",
    "    labels = list( data[\"Non-Redundancy\"] )\n",
    "    print( \"Building and training models...\" )\n",
    "    model = MLPRegressor( ( 10, 3 ), activation = \"tanh\",\n",
    "                          max_iter = 1000, learning_rate_init = 0.01 )\n",
    "    model.fit( features, labels )\n",
    "    return model\n",
    "\n",
    "\"\"\"Test part of problem 4.1.1\n",
    "\n",
    "Args:\n",
    "    data: a dataframe contains Summary text, Non-Redundancy score,\n",
    "          and Fluency score.\n",
    "    model: a trained model.\n",
    "    stop_words: a set of stop words.\n",
    "    word2vec: a word2vec model obtained from nltk.\n",
    "\n",
    "Returns:\n",
    "    None.\n",
    "\"\"\"\n",
    "def problem4_1_1_test( data, model, stop_words, word2vec ):\n",
    "    print( \"Problem 4.1.1 Test\" )\n",
    "    print( \"Getting features...\" )\n",
    "    features = list( zip( get_max_unigram( data, stop_words ),\n",
    "                          get_max_bigram( data, stop_words ),\n",
    "                          get_max_sentence_similarity( data, stop_words,\n",
    "                                                       word2vec ) ) )\n",
    "    labels = list( data[\"Non-Redundancy\"] )\n",
    "    print( \"Predicting...\" )\n",
    "    preds = model.predict( features )\n",
    "    print( \"Measuring...\" )\n",
    "    mses = mean_squared_error( labels, preds )\n",
    "    pcor = pearsonr( labels, preds )\n",
    "    print( mses, pcor )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing...\n",
      "Problem 4.1.1 Train\n",
      "Getting features...\n",
      "Geting maximum repetition of unigrams of each sentence...\n",
      "Geting maximum repetition of bigrams of each sentence...\n",
      "Geting maximum sentence similarity of each sentence...\n",
      "Calculating cosine similarity...\n",
      "Building and training models...\n",
      "Preprocessing...\n",
      "Problem 4.1.1 Test\n",
      "Getting features...\n",
      "Geting maximum repetition of unigrams of each sentence...\n",
      "Geting maximum repetition of bigrams of each sentence...\n",
      "Geting maximum sentence similarity of each sentence...\n",
      "Calculating cosine similarity...\n",
      "Predicting...\n",
      "Measuring...\n",
      "0.23029351476520904 (0.6834279551731149, 7.177369025815646e-29)\n"
     ]
    }
   ],
   "source": [
    "preprocessing( data_train )\n",
    "model_4_1_1 = problem4_1_1_train( data_train, stop_words, word2vec )\n",
    "preprocessing( data_test )\n",
    "problem4_1_1_test( data_test, model_4_1_1, stop_words, word2vec )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "def problem4_1_2_train( data, stop_words, word2vec, func ):\n",
    "    print( \"Problem 4.1.2 Train\" )\n",
    "    print( \"Getting features...\" )\n",
    "    if func == \"length\":\n",
    "        features = list( zip( get_max_unigram( data, stop_words ),\n",
    "                              get_max_bigram( data, stop_words ),\n",
    "                              get_max_sentence_similarity( data, stop_words,\n",
    "                                                           word2vec ),\n",
    "                              get_length( data ) ) )\n",
    "    else:\n",
    "        features = list( zip( get_max_unigram( data, stop_words ),\n",
    "                              get_max_bigram( data, stop_words ),\n",
    "                              get_max_sentence_similarity( data, stop_words,\n",
    "                                                           word2vec ),\n",
    "                              get_ratio_of_stop_words( data, stop_words ) ) )\n",
    "    labels = list( data[\"Non-Redundancy\"] )\n",
    "    print( \"Building and training models...\" )\n",
    "    model = MLPRegressor( ( 10, 3 ), activation = \"tanh\",\n",
    "                           max_iter = 1000, learning_rate_init = 0.01 )\n",
    "    model.fit( features, labels )\n",
    "    return model\n",
    "\n",
    "def problem4_1_2_test( data, model, stop_words, word2vec, func ):\n",
    "    print( \"Problem 4.1.2 Test\" )\n",
    "    print( \"Getting features...\" )\n",
    "    if func == \"length\":\n",
    "        features = list( zip( get_max_unigram( data, stop_words ),\n",
    "                              get_max_bigram( data, stop_words ),\n",
    "                              get_max_sentence_similarity( data, stop_words,\n",
    "                                                           word2vec ),\n",
    "                              get_length( data ) ) )\n",
    "    else:\n",
    "        features = list( zip( get_max_unigram( data, stop_words ),\n",
    "                              get_max_bigram( data, stop_words ),\n",
    "                              get_max_sentence_similarity( data, stop_words,\n",
    "                                                           word2vec ),\n",
    "                              get_ratio_of_stop_words( data, stop_words ) ) )\n",
    "    labels = list( data[\"Non-Redundancy\"] )\n",
    "    print( \"Predicting by model...\" )\n",
    "    preds = model.predict( features )\n",
    "    print( \"Measuring...\" )\n",
    "    mses = mean_squared_error( labels, preds )\n",
    "    pcor = pearsonr( labels, preds )\n",
    "    print( mses, pcor )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Try to add feature length\n",
      "Preprocessing...\n",
      "Problem 4.1.2 Train\n",
      "Getting features...\n",
      "Geting maximum repetition of unigrams of each sentence...\n",
      "Geting maximum repetition of bigrams of each sentence...\n",
      "Geting maximum sentence similarity of each sentence...\n",
      "Calculating cosine similarity...\n",
      "Getting length of each sentence...\n",
      "Building and training models...\n",
      "Preprocessing...\n",
      "Problem 4.1.2 Test\n",
      "Getting features...\n",
      "Geting maximum repetition of unigrams of each sentence...\n",
      "Geting maximum repetition of bigrams of each sentence...\n",
      "Geting maximum sentence similarity of each sentence...\n",
      "Calculating cosine similarity...\n",
      "Getting length of each sentence...\n",
      "Predicting by model...\n",
      "Measuring...\n",
      "0.40044965179218933 (0.057959911971320874, 0.4149436630978265)\n",
      "Try to add feature stop_words\n",
      "Preprocessing...\n",
      "Problem 4.1.2 Train\n",
      "Getting features...\n",
      "Geting maximum repetition of unigrams of each sentence...\n",
      "Geting maximum repetition of bigrams of each sentence...\n",
      "Geting maximum sentence similarity of each sentence...\n",
      "Calculating cosine similarity...\n",
      "Getting ratio of stop words of each sentence...\n",
      "Building and training models...\n",
      "Preprocessing...\n",
      "Problem 4.1.2 Test\n",
      "Getting features...\n",
      "Geting maximum repetition of unigrams of each sentence...\n",
      "Geting maximum repetition of bigrams of each sentence...\n",
      "Geting maximum sentence similarity of each sentence...\n",
      "Calculating cosine similarity...\n",
      "Getting ratio of stop words of each sentence...\n",
      "Predicting by model...\n",
      "Measuring...\n",
      "0.21815957555766785 (0.6902339867508294, 1.2325001975224483e-29)\n"
     ]
    }
   ],
   "source": [
    "func = \"length\"\n",
    "print( \"Try to add feature \" + func )\n",
    "preprocessing( data_train )\n",
    "model_4_1_2_1 = problem4_1_2_train( data_train, stop_words, word2vec, func )\n",
    "preprocessing( data_test )\n",
    "problem4_1_2_test( data_test, model_4_1_2_1, stop_words, word2vec, func )\n",
    "func = \"stop_words\"\n",
    "print( \"Try to add feature \" + func )\n",
    "preprocessing( data_train )\n",
    "model_4_1_2_2 = problem4_1_2_train( data_train, stop_words, word2vec, func )\n",
    "preprocessing( data_test )\n",
    "problem4_1_2_test( data_test, model_4_1_2_2, stop_words, word2vec, func )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
